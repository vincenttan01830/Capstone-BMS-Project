{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in merged_df:\n",
      " time           0\n",
      "temperature    1\n",
      "Temperature    0\n",
      "Humidity       0\n",
      "hour           0\n",
      "day            0\n",
      "month          0\n",
      "dtype: int64\n",
      "                       time  temperature  Temperature  Humidity  hour  day  \\\n",
      "0 2023-03-15 15:00:00+08:00    20.238870           31      0.52    15   15   \n",
      "1 2023-03-15 16:00:00+08:00    20.248299           31      0.52    16   15   \n",
      "2 2023-03-15 17:00:00+08:00    20.249218           30      0.55    17   15   \n",
      "3 2023-03-15 18:00:00+08:00    20.248273           30      0.59    18   15   \n",
      "4 2023-03-15 19:00:00+08:00    20.172554           29      0.58    19   15   \n",
      "\n",
      "   month  \n",
      "0      3  \n",
      "1      3  \n",
      "2      3  \n",
      "3      3  \n",
      "4      3  \n",
      "Column names: Index(['time', 'temperature', 'Temperature', 'Humidity', 'hour', 'day',\n",
      "       'month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined climate and device data\n",
    "climate_data = pd.read_csv('training/clean/combined_climate_data.csv')\n",
    "device_data = pd.read_csv('training/clean/combined_device_data.csv')\n",
    "\n",
    "# Convert the 'Time' and 'time' columns to datetime format\n",
    "climate_data['Time'] = pd.to_datetime(climate_data['Time'])\n",
    "device_data['time'] = pd.to_datetime(device_data['time'])\n",
    "\n",
    "# Merge the datasets on the timestamp\n",
    "merged_df = pd.merge(device_data, climate_data, left_on='time', right_on='Time', how='inner')\n",
    "\n",
    "# Drop the redundant 'Time' column\n",
    "merged_df.drop(columns=['Time'], inplace=True)\n",
    "\n",
    "# Add time-based features\n",
    "merged_df['hour'] = merged_df['time'].dt.hour\n",
    "merged_df['day'] = merged_df['time'].dt.day\n",
    "merged_df['month'] = merged_df['time'].dt.month\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"NaN values in merged_df:\\n\", merged_df.isna().sum())\n",
    "\n",
    "# Drop rows with NaN values or handle them appropriately\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "# Print the merged dataframe\n",
    "print(merged_df.head())\n",
    "print(\"Column names:\", merged_df.columns)\n",
    "\n",
    "# Select features and target variable\n",
    "features = ['Temperature', 'Humidity', 'hour', 'day', 'month', 'temperature']  # Include device temperature in features\n",
    "target = 'temperature'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in scaled_features: 0\n",
      "NaN values in scaled_target: 0\n",
      "NaN values in X: 0\n",
      "NaN values in y: 0\n",
      "NaN values in X_train: 0\n",
      "NaN values in X_test: 0\n",
      "NaN values in y_train: 0\n",
      "NaN values in y_test: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Scale the features and target separately\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaled_features = feature_scaler.fit_transform(merged_df[features])\n",
    "scaled_target = target_scaler.fit_transform(merged_df[[target]])\n",
    "\n",
    "# Check for NaN values in scaled features and target\n",
    "print(\"NaN values in scaled_features:\", np.isnan(scaled_features).sum())\n",
    "print(\"NaN values in scaled_target:\", np.isnan(scaled_target).sum())\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "def create_sequences(features, target, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(features) - seq_length):\n",
    "        x = features[i:i+seq_length]\n",
    "        y = target[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 10\n",
    "X, y = create_sequences(scaled_features, scaled_target, seq_length)\n",
    "\n",
    "# Check for NaN values in sequences\n",
    "print(\"NaN values in X:\", np.isnan(X).sum())\n",
    "print(\"NaN values in y:\", np.isnan(y).sum())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], seq_length, len(features)))\n",
    "X_test = X_test.reshape((X_test.shape[0], seq_length, len(features)))\n",
    "\n",
    "# Check for NaN values in training and testing sets\n",
    "print(\"NaN values in X_train:\", np.isnan(X_train).sum())\n",
    "print(\"NaN values in X_test:\", np.isnan(X_test).sum())\n",
    "print(\"NaN values in y_train:\", np.isnan(y_train).sum())\n",
    "print(\"NaN values in y_test:\", np.isnan(y_test).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 2s 25ms/step - loss: 0.1503 - val_loss: 0.0735\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.0794 - val_loss: 0.0593\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0578 - val_loss: 0.0504\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0476 - val_loss: 0.0401\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0458 - val_loss: 0.0366\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0396 - val_loss: 0.0396\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.0364 - val_loss: 0.0347\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0322 - val_loss: 0.0308\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.0349 - val_loss: 0.0302\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0301 - val_loss: 0.0247\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0278 - val_loss: 0.0240\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0278 - val_loss: 0.0248\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0252 - val_loss: 0.0202\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0224 - val_loss: 0.0187\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0226 - val_loss: 0.0194\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0228 - val_loss: 0.0173\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0213 - val_loss: 0.0180\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0219 - val_loss: 0.0164\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0228 - val_loss: 0.0159\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0177 - val_loss: 0.0135\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.0185 - val_loss: 0.0141\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0168 - val_loss: 0.0143\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0162 - val_loss: 0.0152\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0186 - val_loss: 0.0194\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0154 - val_loss: 0.0117\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0149 - val_loss: 0.0148\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0151 - val_loss: 0.0127\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0138 - val_loss: 0.0123\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0144\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0137 - val_loss: 0.0107\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0185 - val_loss: 0.0099\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0134 - val_loss: 0.0121\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0124 - val_loss: 0.0161\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0128 - val_loss: 0.0101\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0127 - val_loss: 0.0114\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0134 - val_loss: 0.0133\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0192\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0118 - val_loss: 0.0105\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0119 - val_loss: 0.0093\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0096 - val_loss: 0.0091\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0080 - val_loss: 0.0163\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0145 - val_loss: 0.0118\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0106 - val_loss: 0.0092\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0116 - val_loss: 0.0081\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0099 - val_loss: 0.0083\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0077 - val_loss: 0.0073\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0079 - val_loss: 0.0066\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0080 - val_loss: 0.0084\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0102 - val_loss: 0.0073\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.0080 - val_loss: 0.0068\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0073 - val_loss: 0.0061\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0080 - val_loss: 0.0059\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0078 - val_loss: 0.0075\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0069 - val_loss: 0.0080\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0092 - val_loss: 0.0069\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0120 - val_loss: 0.0176\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0100 - val_loss: 0.0073\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0125 - val_loss: 0.0073\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0084 - val_loss: 0.0081\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0074 - val_loss: 0.0062\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0060 - val_loss: 0.0063\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0060 - val_loss: 0.0065\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0080 - val_loss: 0.0087\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0067 - val_loss: 0.0058\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0055 - val_loss: 0.0068\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0109\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0062\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0050 - val_loss: 0.0065\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0060 - val_loss: 0.0069\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0082 - val_loss: 0.0095\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0059 - val_loss: 0.0065\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0084 - val_loss: 0.0066\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0068 - val_loss: 0.0076\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0066 - val_loss: 0.0062\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0072 - val_loss: 0.0107\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0061 - val_loss: 0.0087\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0054 - val_loss: 0.0065\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0061\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0057\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0062\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0066 - val_loss: 0.0083\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0056 - val_loss: 0.0067\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0105 - val_loss: 0.0084\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0081 - val_loss: 0.0061\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0052 - val_loss: 0.0055\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0052 - val_loss: 0.0061\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0061\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0060\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0052 - val_loss: 0.0053\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0045 - val_loss: 0.0056\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0052 - val_loss: 0.0063\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.0042 - val_loss: 0.0076\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define a more complex LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(128, activation='relu', return_sequences=True, input_shape=(seq_length, len(features))),  # Increased number of units\n",
    "    Dropout(0.3),  # Increased dropout rate\n",
    "    LSTM(64, activation='relu'),  # Second LSTM layer with more units\n",
    "    Dropout(0.3),  # Increased dropout rate\n",
    "    Dense(32, activation='relu'),  # Added a Dense layer\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model with a different optimizer and learning rate\n",
    "optimizer = Adam(learning_rate=0.001)  # Specifying learning rate\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Train the model with modified batch size and number of epochs\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0200\n",
      "Test Loss with all features: 0.020013101398944855\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "NaN in y_pred: 0\n",
      "y_test_filtered shape: (146, 1)\n",
      "y_pred_filtered shape: (146, 1)\n",
      "Mean Absolute Error with all features: 0.6287282043726254\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss with all features: {test_loss}')\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse scale the predictions and target values\n",
    "y_pred = target_scaler.inverse_transform(y_pred)\n",
    "y_test = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "# Check for NaN values in predictions\n",
    "print(f\"NaN in y_pred: {np.isnan(y_pred).sum()}\")\n",
    "\n",
    "# Remove NaN values\n",
    "mask = ~np.isnan(y_pred).flatten()\n",
    "y_test_filtered = y_test[mask]\n",
    "y_pred_filtered = y_pred[mask]\n",
    "\n",
    "# Debugging: Check shapes of y_test_filtered and y_pred_filtered\n",
    "print(\"y_test_filtered shape:\", y_test_filtered.shape)\n",
    "print(\"y_pred_filtered shape:\", y_pred_filtered.shape)\n",
    "\n",
    "# Calculate mean absolute error (MAE)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test_filtered, y_pred_filtered)\n",
    "print(f'Mean Absolute Error with all features: {mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15/15 [==============================] - 2s 24ms/step - loss: 0.1634 - val_loss: 0.0919\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0932 - val_loss: 0.0915\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0842 - val_loss: 0.0996\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0775 - val_loss: 0.0831\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0676 - val_loss: 0.0820\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0647 - val_loss: 0.0860\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0625 - val_loss: 0.0809\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0576 - val_loss: 0.0813\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0583 - val_loss: 0.0855\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0587 - val_loss: 0.0816\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0568 - val_loss: 0.0823\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0581 - val_loss: 0.0785\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0581 - val_loss: 0.0771\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0582 - val_loss: 0.0779\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0571 - val_loss: 0.0769\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0572 - val_loss: 0.0785\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0547 - val_loss: 0.0803\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0600 - val_loss: 0.0755\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0557 - val_loss: 0.0771\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0539 - val_loss: 0.0761\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.0532 - val_loss: 0.0784\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0562 - val_loss: 0.0795\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0516 - val_loss: 0.0806\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0538 - val_loss: 0.0857\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0528 - val_loss: 0.0817\n",
      "Epoch 26/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0518 - val_loss: 0.0773\n",
      "Epoch 27/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0536 - val_loss: 0.0784\n",
      "Epoch 28/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0587 - val_loss: 0.0751\n",
      "Epoch 29/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0550 - val_loss: 0.0788\n",
      "Epoch 30/30\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.0519 - val_loss: 0.0797\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0724\n",
      "Test Loss without past device temperature: 0.07238898426294327\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "Mean Absolute Error without past device temperature: 1.9734119480575436\n"
     ]
    }
   ],
   "source": [
    "# Select features without past device temperature\n",
    "features_no_device_temp = ['Temperature', 'Humidity', 'hour', 'day', 'month']\n",
    "scaled_features_no_device_temp = feature_scaler.fit_transform(merged_df[features_no_device_temp])\n",
    "\n",
    "# Prepare sequences for LSTM without device temperature\n",
    "X_no_device_temp, y_no_device_temp = create_sequences(scaled_features_no_device_temp, scaled_target, seq_length)\n",
    "\n",
    "# Split the data\n",
    "X_train_no_device_temp, X_test_no_device_temp, y_train_no_device_temp, y_test_no_device_temp = train_test_split(\n",
    "    X_no_device_temp, y_no_device_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input\n",
    "X_train_no_device_temp = X_train_no_device_temp.reshape((X_train_no_device_temp.shape[0], seq_length, len(features_no_device_temp)))\n",
    "X_test_no_device_temp = X_test_no_device_temp.reshape((X_test_no_device_temp.shape[0], seq_length, len(features_no_device_temp)))\n",
    "\n",
    "# Define and compile the model\n",
    "model_no_device_temp = Sequential([\n",
    "    LSTM(128, activation='relu', return_sequences=True, input_shape=(seq_length, len(features_no_device_temp))),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model_no_device_temp.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history_no_device_temp = model_no_device_temp.fit(X_train_no_device_temp, y_train_no_device_temp, epochs=30, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_no_device_temp = model_no_device_temp.evaluate(X_test_no_device_temp, y_test_no_device_temp)\n",
    "print(f'Test Loss without past device temperature: {test_loss_no_device_temp}')\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_no_device_temp = model_no_device_temp.predict(X_test_no_device_temp)\n",
    "\n",
    "# Inverse scale the predictions and target values\n",
    "y_pred_no_device_temp = target_scaler.inverse_transform(y_pred_no_device_temp)\n",
    "y_test_no_device_temp = target_scaler.inverse_transform(y_test_no_device_temp)\n",
    "\n",
    "# Calculate mean absolute error (MAE)\n",
    "mae_no_device_temp = mean_absolute_error(y_test_no_device_temp, y_pred_no_device_temp)\n",
    "print(f'Mean Absolute Error without past device temperature: {mae_no_device_temp}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15/15 [==============================] - 2s 20ms/step - loss: 0.1674 - val_loss: 0.0734\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0820 - val_loss: 0.0588\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0616 - val_loss: 0.0525\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0490 - val_loss: 0.0402\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0465 - val_loss: 0.0354\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0423 - val_loss: 0.0365\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0386 - val_loss: 0.0442\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0327 - val_loss: 0.0358\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0347 - val_loss: 0.0367\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0350 - val_loss: 0.0296\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0314 - val_loss: 0.0370\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0328 - val_loss: 0.0354\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0326 - val_loss: 0.0235\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0282 - val_loss: 0.0271\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0278 - val_loss: 0.0282\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0276 - val_loss: 0.0227\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0279 - val_loss: 0.0306\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0250 - val_loss: 0.0226\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0286 - val_loss: 0.0289\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0270 - val_loss: 0.0222\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0234 - val_loss: 0.0189\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0255 - val_loss: 0.0210\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0232 - val_loss: 0.0202\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0215 - val_loss: 0.0178\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0245 - val_loss: 0.0182\n",
      "Epoch 26/30\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.0218 - val_loss: 0.0170\n",
      "Epoch 27/30\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.0226 - val_loss: 0.0161\n",
      "Epoch 28/30\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0222 - val_loss: 0.0186\n",
      "Epoch 29/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0218 - val_loss: 0.0176\n",
      "Epoch 30/30\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0200 - val_loss: 0.0148\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0250\n",
      "Test Loss without climate temperature: 0.025045743212103844\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Mean Absolute Error without climate temperature: 1.0664309207319869\n"
     ]
    }
   ],
   "source": [
    "# Select features without climate temperature\n",
    "features_no_climate_temp = ['hour', 'day', 'month', 'temperature']\n",
    "scaled_features_no_climate_temp = feature_scaler.fit_transform(merged_df[features_no_climate_temp])\n",
    "\n",
    "# Prepare sequences for LSTM without climate temperature\n",
    "X_no_climate_temp, y_no_climate_temp = create_sequences(scaled_features_no_climate_temp, scaled_target, seq_length)\n",
    "\n",
    "# Split the data\n",
    "X_train_no_climate_temp, X_test_no_climate_temp, y_train_no_climate_temp, y_test_no_climate_temp = train_test_split(\n",
    "    X_no_climate_temp, y_no_climate_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input\n",
    "X_train_no_climate_temp = X_train_no_climate_temp.reshape((X_train_no_climate_temp.shape[0], seq_length, len(features_no_climate_temp)))\n",
    "X_test_no_climate_temp = X_test_no_climate_temp.reshape((X_test_no_climate_temp.shape[0], seq_length, len(features_no_climate_temp)))\n",
    "\n",
    "# Define and compile the model\n",
    "model_no_climate_temp = Sequential([\n",
    "    LSTM(128, activation='relu', return_sequences=True, input_shape=(seq_length, len(features_no_climate_temp))),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model_no_climate_temp.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history_no_climate_temp = model_no_climate_temp.fit(X_train_no_climate_temp, y_train_no_climate_temp, epochs=30, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_no_climate_temp = model_no_climate_temp.evaluate(X_test_no_climate_temp, y_test_no_climate_temp)\n",
    "print(f'Test Loss without climate temperature: {test_loss_no_climate_temp}')\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_no_climate_temp = model_no_climate_temp.predict(X_test_no_climate_temp)\n",
    "\n",
    "# Inverse scale the predictions and target values\n",
    "y_pred_no_climate_temp = target_scaler.inverse_transform(y_pred_no_climate_temp)\n",
    "y_test_no_climate_temp = target_scaler.inverse_transform(y_test_no_climate_temp)\n",
    "\n",
    "# Calculate mean absolute error (MAE)\n",
    "mae_no_climate_temp = mean_absolute_error(y_test_no_climate_temp, y_pred_no_climate_temp)\n",
    "print(f'Mean Absolute Error without climate temperature: {mae_no_climate_temp}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
